{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q keras_nlp\n%pip install -q tflite_runtime\n%pip install -q einops\n\nimport gc\ninterpolated_length = 19 #mean length /2","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:05.880742Z","iopub.execute_input":"2023-04-10T07:28:05.881028Z","iopub.status.idle":"2023-04-10T07:28:48.456639Z","shell.execute_reply.started":"2023-04-10T07:28:05.881000Z","shell.execute_reply":"2023-04-10T07:28:48.455096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nfrom os.path import join as pjoin\n\nROWS_PER_FRAME = 543  # number of landmarks per frame\n\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)\n\ndef tf_get_features(ftensor):\n    def feat_wrapper(ftensor):\n        return load_relevant_data_subset(ftensor.numpy().decode('utf-8'))\n    return tf.py_function(\n        feat_wrapper,\n        [ftensor],\n        Tout=tf.float32\n    )\n\ndef tf_nan_mean(x, axis=0, keepdims=True):\n    return (tf.reduce_sum(\n        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), \n        axis=axis, keepdims=keepdims) \n        / tf.reduce_sum(\n            tf.where(\n                tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), \n            axis=axis, keepdims=keepdims))\n\ndef tf_nan_std(x, axis=0, keepdims=True):\n    d = x - tf_nan_mean(x, axis=axis, keepdims=keepdims)\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return \"%dm %ds\" % (m, s)\n\ndef timeSpent(since):\n    now = time.time()\n    s = now - since\n    return asMinutes(s)\n\nclass TimeLimitCallback(tf.keras.callbacks.Callback):\n    def __init__(self, start_time, max_duration_hours=8, max_duration_minutes=30):\n        super(TimeLimitCallback, self).__init__()\n        self.start_time = start_time\n        self.max_duration_seconds = max_duration_hours * 3600 + max_duration_minutes * 60\n\n    def on_train_batch_end(self, batch, logs=None):\n        elapsed_time = time.time() - self.start_time\n        if elapsed_time > self.max_duration_seconds:\n            self.model.stop_training = True\n            print(f\"Training stopped: time limit of {self.max_duration_seconds/3600:.1f} hours exceeded\")\n\n@tf.autograph.experimental.do_not_convert\ndef detuple(v, l, g, pid, sid):\n    return (v, l)\n\n@tf.autograph.experimental.do_not_convert\ndef ensure(shape):\n    return lambda x : tf.ensure_shape(x, shape)\n\ndef scheduler(epoch, lr):\n    if epoch < 8:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n    \ndef create_load_tfrecords(new=False):\n    idx = 0\n    while f\"run_{idx}\" in os.listdir('.'):\n        idx +=1\n\n    if new:\n        path = f\"run_{idx}\"\n        print(f\"Results will be saved at path {path}\")\n        os.makedirs(path)\n        # !mkdir $path\n\n    else:\n        path = f\"run_{idx-1}\"\n        print(f\"Using path {path}\")\n    \n    return pjoin(path, 'full_cv.tfrecord')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:48.460460Z","iopub.execute_input":"2023-04-10T07:28:48.460933Z","iopub.status.idle":"2023-04-10T07:28:54.617892Z","shell.execute_reply.started":"2023-04-10T07:28:48.460878Z","shell.execute_reply":"2023-04-10T07:28:54.616814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntfkl = tf.keras.layers\n\n#import imageio\n#from PIL import Image\n#import matplotlib.pyplot as plt\n\nclass Preprocess(tf.keras.layers.Layer):\n    '''\n    This is the first version of Preprocessing layer with visualisation of preprocessed data as gifs.\n    - 4 modalities : pose, eyes, mouth, and normalized hand.\n    - Each modality is individually time-interpolated according to interpolated_length, at the timesteps \n        having data for this modality.\n    - A problem might happen for samples having less than 'interpolated_length' available timesteps.\n    - Besides, it might be hard for the model to account for different modalities sampled at different\n        timesteps.\n    '''\n    def __init__(self, interpolated_length=10):\n        super(Preprocess, self).__init__()\n        self.fixed_frames = interpolated_length\n\n        self.mod2indexes = {\n            \"pose\": tf.constant([468, 500, 501, 502, 503, 522]), \n            \"hands\": tf.constant([\n                [468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488], \n                [522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542]\n            ]), \n            \"eyes\": tf.constant([7, 33, 133, 144, 145, 153, 154, 155, 157, 158, 159, 160, 161, 163, 173, 246, 249, 263, 362, 373, 374, 380, 381, 382, 384, 385, 386, 387, 388, 390, 398, 466]), \n            \"mouth\": tf.constant([13, 14, 78, 80, 81, 82, 87, 88, 95, 178, 191, 308, 310, 311, 312, 317, 318, 324, 402, 415])\n        }\n\n        self.lh_idx_range=(468, 489)\n        self.rh_idx_range=(522, 543)\n\n        self.mod2edge = {\n            \"pose\": np.array([\n                [4, 3, 1, 0, 2], #for right handed\n                [4, 2, 1, 3, 0]  #for left handed\n            ]),\n            \"hands\": np.array([\n                [0, 1, 2, 3, 4],\n                [0, 5, 6, 7, 8],\n                [0, 17, 18, 19, 20],\n                [17, 13, 14, 15, 16],\n                [5, 9, 10, 11, 12],\n            ]), \n            \"eyes\": np.array([\n                [1, 15, 12, 11, 10, 9, 8, 14, 2, 7, 6, 5, 4, 3, 13, 0, 1],\n                [18, 30, 24, 25, 26, 27, 28, 31, 17, 16, 29, 19, 20, 21, 22, 23, 18]\n            ]),\n            \"mouth\": np.array([\n                [2, 10, 3, 4, 5, 0, 14, 13, 12, 19, 11, 17, 16, 18, 15, 1, 6, 9, 7, 8]\n            ])\n        }\n\n        self.mod2style = {\n            \"pose\": ('black', 3),\n            \"hands\": ('blue', 2), \n            \"eyes\": ('black', 1),\n            \"mouth\": ('red', 1)\n        }\n    \n    @tf.function\n    def pad_or_truncate_center(self, tensor, fixed_length):\n        seq_length = tf.shape(tensor)[0]\n        \n        if seq_length > fixed_length:\n            # Truncate the sequence by selecting the center patch\n            start_idx = (seq_length - fixed_length) // 2\n            tensor = tensor[start_idx:start_idx + fixed_length, :, :]\n        elif seq_length < fixed_length:\n            # Pad the sequence with zeros\n            padding_length = fixed_length - seq_length\n            padding = tf.zeros((padding_length, tf.shape(tensor)[1], tf.shape(tensor)[2]))\n            tensor = tf.concat([tensor, padding], axis=0)\n\n        return tensor\n\n    def call(self, tensor):\n        tensor = tensor[:, :, :2]  # drop z axis\n        do_symetry = (tf.reduce_mean(tf.cast(tf.math.is_nan(tensor[:, self.rh_idx_range[0]:self.rh_idx_range[1], :]), tf.float32))\n                    < tf.reduce_mean(tf.cast(tf.math.is_nan(tensor[:, self.lh_idx_range[0]:self.lh_idx_range[1], :]), tf.float32)))\n        \n        tensor = tf.cond(# symetry on 2nd axis when left-handed\n            tf.equal(do_symetry, True),\n            lambda: tf.stack([1 - tensor[:, :, 0], tensor[:, :, 1]], axis=-1),\n            lambda: tensor\n        )\n        #if do_symetry:  # symetry on 2nd axis when left-handed\n        #    tensor = tf.stack([1 - tensor[:, :, 0], tensor[:, :, 1]], axis=-1)\n\n        modalities, mod_indexes = {}, {}\n        for modality, modalitidx in self.mod2indexes.items():\n            mod_tensor = tf.gather(tensor, modalitidx, axis=1)\n            fallback_shape = tf.shape(mod_tensor)\n            \n            if modality == \"pose\":\n                mod_tensor = tf.cond(\n                    do_symetry,\n                    lambda: mod_tensor[:, 1:],\n                    lambda: mod_tensor[:, :-1]\n                )\n            if modality == \"hands\":\n                mod_tensor = tf.cond(\n                    tf.equal(do_symetry, True),\n                    lambda: mod_tensor[:, 1],\n                    lambda: mod_tensor[:, 0]\n                )\n\n            #if modality == \"pose\":\n            #    mod_tensor = mod_tensor[:, 1:] if do_symetry else mod_tensor[:, :-1]\n            #elif modality == \"hands\":\n            #    mod_tensor = mod_tensor[:, tf.cast(do_symetry, tf.int32)]\n\n            # Replacing loop with tensor operations\n            nan_frames_mask = tf.reduce_all(tf.math.is_nan(mod_tensor), axis=[1, 2])\n            nan_frames_mask = tf.reshape(nan_frames_mask, [-1])\n            \n            mod_tensor = tf.boolean_mask(mod_tensor, tf.math.logical_not(nan_frames_mask), axis=0)\n            mod_tensor = tf.where(tf.math.is_nan(mod_tensor), tf.zeros_like(mod_tensor), mod_tensor)\n\n            # if tf.shape(mod_tensor)[0] == 0:\n            #     fallback_shape = tf.concat([tf.constant([1]), fallback_shape[1:]], axis=0)\n            #     mod_tensor = tf.zeros(fallback_shape)\n\n            # mod_n = tf.shape(mod_tensor)[0]\n            # mod_n = tf.reduce_max(tf.stack([mod_n, 1]))\n            # mod_n = tf.stack([self.fixed_frames - 1, mod_n])\n            # mod_n = tf.reduce_min(mod_n)\n            # mod_time_interpolated = tf.cast(tf.linspace(0, mod_n - 1, self.fixed_frames), tf.int32)\n\n            # mod_tensor = tf.gather(mod_tensor, mod_time_interpolated, axis=0)\n\n            mod_tensor = self.pad_or_truncate_center(mod_tensor, self.fixed_frames)\n\n            modalities[modality] = mod_tensor\n            # mod_indexes[modality] = mod_time_interpolated\n\n        return modalities\n        # return modalities, mod_indexes\n  \n    def plot_sketch(self, array, modality):\n        modalitidx = self.mod2edge[modality]\n        col, width = self.mod2style[modality]\n        for j, seg in enumerate(modalitidx):\n            seg = np.array(seg)\n            plt.plot(array[seg, 0], -array[seg, 1], '-', color=col, linewidth=width)    \n        \n    def output_gif_result(self, tensor, out_path='untitled.gif', gif_size=(224, 224)):\n        is_right_handed = (tf.reduce_mean(tf.cast(tf.math.is_nan(tensor[0, self.rh_idx_range[0]:self.rh_idx_range[1], :2]), tf.float32)) \n                        < tf.reduce_mean(tf.cast(tf.math.is_nan(tensor[0, self.lh_idx_range[0]:self.lh_idx_range[1], :2]), tf.float32)))\n        \n        mod2tensor = self.call(tensor)[0]\n        mod2tensor = {k:v.numpy() for k,v in mod2tensor.items()}\n\n        for pose, hands in zip(mod2tensor['pose'], mod2tensor['hands']):\n            if is_right_handed:\n                pose[-1] = hands[0]\n                self.mod2edge[\"pose\"] = self.mod2edge[\"pose\"][:1]\n            else:\n                pose[0] = hands[0]\n                self.mod2edge[\"pose\"] = self.mod2edge[\"pose\"][-1:]\n\n        images = []\n\n        times = [{} for _ in range(self.fixed_frames)]\n        for mod, arrays in mod2tensor.items():\n            for t, array in enumerate(arrays):\n                times[t][mod] = array\n\n        for i, time in enumerate(times):\n            plt.figure(figsize=(gif_size[0]/100, gif_size[1]/100))\n            for modality in time.keys():\n                modalitidx = self.mod2edge[modality]\n                col, width = self.mod2style[modality]\n                array = time[modality]\n                for j, seg in enumerate(modalitidx):\n                    seg = np.array(seg)\n                    plt.plot(array[seg, 0], -array[seg, 1], '-', color=col, linewidth=width)\n            plt.xlim(-0.25, 1.25)\n            plt.ylim(-1.5, 0)\n            plt.axis('off')\n            plt.gca().set_position([0, 0, 1, 1])\n            plt.savefig(f\"tmp/frame_{i:02d}.png\")\n            plt.close()\n            images.append(Image.open(f\"tmp/frame_{i:02d}.png\"))\n\n        images[0].save(\n            out_path,\n            save_all=True,\n            append_images=images[1:],\n            duration=100,\n            loop=0,\n            size=gif_size)\n        \n        # Save the images as a GIF file\n        imageio.mimsave(out_path, images, fps=5)\n        return out_path","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:54.619624Z","iopub.execute_input":"2023-04-10T07:28:54.620304Z","iopub.status.idle":"2023-04-10T07:28:54.657703Z","shell.execute_reply.started":"2023-04-10T07:28:54.620272Z","shell.execute_reply":"2023-04-10T07:28:54.656583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os.path import join as pjoin\nfrom os.path import exists\nimport os\nfrom sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport numpy as np\nimport json\n\ntry :\n    BASE_DIR = \"/kaggle/input/asl-signs/\"\n    df = pd.read_csv(pjoin(BASE_DIR, \"train.csv\"))\nexcept :\n    BASE_DIR = \"asl-signs\"\n    df = pd.read_csv(pjoin(BASE_DIR, \"train.csv\"))\n\npath2label = dict(zip(df.path, df.sign))\nlabel2int = json.load(open(pjoin(BASE_DIR, \"sign_to_prediction_index_map.json\"), 'rb'))\n\n# KFold\ndef get_KFold_dataset(preprocessing, path='untitled', n_splits=3):\n    dir_path = path\n    path = pjoin(path, 'full_cv.tfrecord')\n\n    def parse_function(example_proto):\n        feature_description = {\n            'pose': tf.io.FixedLenFeature(shape=(interpolated_length, 5, 2), dtype=tf.float32),\n            'hands': tf.io.FixedLenFeature(shape=(interpolated_length, 21, 2), dtype=tf.float32),\n            'eyes': tf.io.FixedLenFeature(shape=(interpolated_length, 32, 2), dtype=tf.float32),\n            'mouth': tf.io.FixedLenFeature(shape=(interpolated_length, 20, 2), dtype=tf.float32),\n\n            # 'pose_idx': tf.io.FixedLenFeature(shape=(10), dtype=tf.int64),\n            # 'hands_idx': tf.io.FixedLenFeature(shape=(10), dtype=tf.int64),\n            # 'eyes_idx': tf.io.FixedLenFeature(shape=(10), dtype=tf.int64),\n            # 'mouth_idx': tf.io.FixedLenFeature(shape=(10), dtype=tf.int64),\n            \n            'label': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n            'group': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n            'pid': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n            'sid': tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n        }\n        parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n        return parsed_example, parsed_example['label']\n\n    if exists(dir_path):\n        print(f\"Reloading dataset from path {path}\")\n    else:\n        print(f\"Dataset will be saved at path {path}\")\n        os.makedirs(dir_path)\n\n        ds = tf.data.TFRecordDataset(path)\n        X_ds = tf.data.Dataset.from_tensor_slices(\n            BASE_DIR + \"/\" + df.path.values\n            ).map(tf_get_features)\n        y_ds = tf.data.Dataset.from_tensor_slices(\n            df.sign.map(label2int).values.reshape(-1,1)\n            )\n        \n        # Perform stratisfied kfold split\n        sgkf = StratifiedGroupKFold(n_splits=n_splits, random_state=42, shuffle=True)\n\n        fold2id = dict()\n        for fold_idx, (index_train, index_valid) in enumerate(sgkf.split(df.path, df.sign, df.participant_id)):\n            fold2id[fold_idx] = np.unique(df.participant_id.values[index_valid])\n            \n        id2fold = dict()\n        for k,v in fold2id.items():\n            for vv in v:\n                id2fold[vv] = k\n\n        g_ds = tf.data.Dataset.from_tensor_slices(\n            df.participant_id.map(id2fold).values.reshape(-1, 1)\n        )\n        pid_ds = tf.data.Dataset.from_tensor_slices(\n            df.participant_id.values\n        )\n        sid_ds = tf.data.Dataset.from_tensor_slices(\n            df.sequence_id.values\n        )\n        \n        with tf.io.TFRecordWriter(path) as writer:\n            zipper = zip(X_ds.map(preprocessing), y_ds, g_ds, pid_ds, sid_ds)\n            for example in zipper:\n                X, y, g, pid, sid = example\n                # X, posX = X\n                feature = {\n                    'pose': tf.train.Feature(float_list=tf.train.FloatList(value=X[\"pose\"].numpy().flatten())),\n                    'hands': tf.train.Feature(float_list=tf.train.FloatList(value=X[\"hands\"].numpy().flatten())),\n                    'eyes': tf.train.Feature(float_list=tf.train.FloatList(value=X[\"eyes\"].numpy().flatten())),\n                    'mouth': tf.train.Feature(float_list=tf.train.FloatList(value=X[\"mouth\"].numpy().flatten())),\n\n                    # 'pose_idx': tf.train.Feature(int64_list=tf.train.Int64List(value=posX[\"pose\"].numpy().flatten())),\n                    # 'hands_idx': tf.train.Feature(int64_list=tf.train.Int64List(value=posX[\"hands\"].numpy().flatten())),\n                    # 'eyes_idx': tf.train.Feature(int64_list=tf.train.Int64List(value=posX[\"eyes\"].numpy().flatten())),\n                    # 'mouth_idx': tf.train.Feature(int64_list=tf.train.Int64List(value=posX[\"mouth\"].numpy().flatten())),\n\n                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=y.numpy().flatten())),\n                    'group': tf.train.Feature(int64_list=tf.train.Int64List(value=g.numpy().flatten())),\n                    'pid': tf.train.Feature(int64_list=tf.train.Int64List(value=pid.numpy().flatten())),\n                    'sid': tf.train.Feature(int64_list=tf.train.Int64List(value=sid.numpy().flatten())),\n                }\n                example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n                writer.write(example_proto.SerializeToString())\n\n    ds = tf.data.TFRecordDataset(path)\n    ds = ds.map(parse_function).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:54.660817Z","iopub.execute_input":"2023-04-10T07:28:54.661323Z","iopub.status.idle":"2023-04-10T07:28:55.365625Z","shell.execute_reply.started":"2023-04-10T07:28:54.661286Z","shell.execute_reply":"2023-04-10T07:28:55.364565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_nlp.layers.transformer_encoder import TransformerEncoder\nfrom keras_nlp.layers import TokenAndPositionEmbedding\nfrom einops import rearrange\nimport tensorflow as tf\ntfkl = tf.keras.layers\n\ndef get_embedder_model(d_model, dropout): \n    #may be better to optimize on the hidden dimension for each modality\n    return tf.keras.Sequential([\n        tfkl.TimeDistributed(tfkl.Flatten()),\n        tfkl.Dense(2*d_model),\n        tfkl.BatchNormalization(),\n        tfkl.Dropout(dropout),\n        tfkl.Activation('relu'),\n        tfkl.Dense(2*d_model),\n        tfkl.BatchNormalization(),\n        tfkl.Dropout(dropout),\n        tfkl.Activation('relu'),\n        tfkl.Dense(d_model),\n        tfkl.BatchNormalization(),\n        tfkl.Dropout(dropout),\n        tfkl.Activation('relu')\n    ])\n\nfrom keras import backend as backend\ndef get_model(hp):\n    backend.clear_session()\n    # 0 mask, 1 eyes, 2 hands, 3 mouth, 4 pose, 5 cls\n    modalities = ['eyes', 'hands', 'mouth', 'pose']\n\n    mod2inputs = {\n        'eyes': tfkl.Input(shape=(interpolated_length, 32, 2), name=\"eyes\"), \n        'hands': tfkl.Input(shape=(interpolated_length, 21, 2), name=\"hands\"), \n        'mouth': tfkl.Input(shape=(interpolated_length, 20, 2), name=\"mouth\"), \n        'pose': tfkl.Input(shape=(interpolated_length, 5, 2), name=\"pose\")\n    }\n\n    # attention mask\n    attention_mask = tf.concat([tf.math.reduce_any(tf.math.logical_not(tf.math.equal(mod2inputs[mod], 0)), axis=[-2, -1]) for mod in modalities], axis=-1)\n    attention_mask = tf.concat([tf.tile(tf.constant([True]), (tf.shape(attention_mask)[0],))[:, tf.newaxis], attention_mask], axis=-1)\n\n    mod2model = {k:get_embedder_model(hp['dim_model'], hp['emb_dropout']) for k in modalities}\n    modalities_embedding = tf.stack([mod2model[mod](mod2inputs[mod]) for mod in modalities], axis=1)\n\n    # mod-space & time encoding\n    tokens = [k*tf.ones_like(mod2inputs[mod][:, :, 0, 0]) for k,mod in enumerate(modalities)]\n    tokens = tf.stack(tokens, axis=1)\n\n    tokens_embedder = TokenAndPositionEmbedding(6, interpolated_length, hp['dim_model'], mask_zero=True)\n    tokens_embedding = tokens_embedder(tokens)\n\n    # add to embedding\n    modalities_ts = modalities_embedding + tokens_embedding\n    modalities_ts = rearrange(modalities_ts, 'b m t d -> b (m t) d')\n\n    cls = tokens_embedder(tf.constant([5]))\n    cls = tf.tile(cls, (tf.shape(modalities_ts)[0], 1))[:, tf.newaxis]\n    full_embedding = tf.concat([cls, modalities_ts], axis=1)\n\n    for _ in range(hp['num_enc']):\n        full_embedding = TransformerEncoder(intermediate_dim=hp['dim_model'], num_heads=hp['nhead'], dropout=hp['t_dropout'])(full_embedding, \n                                                                                                                              padding_mask=attention_mask)\n\n    pool = full_embedding[:, 0]\n\n    clas = tfkl.Dense(250, 'softmax')(pool)\n\n    model = tf.keras.Model(inputs=list(mod2inputs.values()), outputs=clas)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:55.367724Z","iopub.execute_input":"2023-04-10T07:28:55.368364Z","iopub.status.idle":"2023-04-10T07:28:55.803345Z","shell.execute_reply.started":"2023-04-10T07:28:55.368324Z","shell.execute_reply":"2023-04-10T07:28:55.802273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os.path import join as pjoin\nimport pandas as pd\nimport numpy as np\nimport pyarrow.parquet as pq\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\ntfkl = tf.keras.layers\nfrom tqdm.keras import TqdmCallback\nimport json\nfrom sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\nimport time\nimport math\nimport os\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:55.804915Z","iopub.execute_input":"2023-04-10T07:28:55.805335Z","iopub.status.idle":"2023-04-10T07:28:56.005136Z","shell.execute_reply.started":"2023-04-10T07:28:55.805283Z","shell.execute_reply":"2023-04-10T07:28:56.004141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mock_run = False","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:56.006387Z","iopub.execute_input":"2023-04-10T07:28:56.006777Z","iopub.status.idle":"2023-04-10T07:28:56.013840Z","shell.execute_reply.started":"2023-04-10T07:28:56.006740Z","shell.execute_reply":"2023-04-10T07:28:56.012686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"preprocessed_datasets/vivit_experiment_centerpad/\"\n\nif not mock_run:\n    ds = get_KFold_dataset(Preprocess(interpolated_length), path)\n    ds = ds.shuffle(int(1e5)).batch(256)\nprint('et zebardi')\n\nhp = {\n    'emb_dropout':0.1,\n    'dim_model':196,#177\n    'num_enc':3,\n    # 'ff_dim':1024,\n    'nhead':8,\n    't_dropout':0.3\n}\n\nfold_idx = 0\n#train_ds = ds.filter(lambda X, y: X['group'] != fold_idx).shuffle(int(1e5)).batch(256)\n#valid_ds = ds.filter(lambda X, y: X['group'] == fold_idx).batch(256)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:56.015467Z","iopub.execute_input":"2023-04-10T07:28:56.016511Z","iopub.status.idle":"2023-04-10T07:28:58.692704Z","shell.execute_reply.started":"2023-04-10T07:28:56.016469Z","shell.execute_reply":"2023-04-10T07:28:58.691596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeLimitCallback(tf.keras.callbacks.Callback):\n    def __init__(self, start_time, max_duration_hours=8, max_duration_minutes=30):\n        super(TimeLimitCallback, self).__init__()\n        self.start_time = start_time\n        self.max_duration_seconds = max_duration_hours * 3600 + max_duration_minutes * 60\n\n    def on_train_batch_end(self, batch, logs=None):\n        elapsed_time = time.time() - self.start_time\n        if elapsed_time > self.max_duration_seconds:\n            self.model.stop_training = True\n            print(f\"Training stopped: time limit of {self.max_duration_seconds/3600:.1f} hours exceeded\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:58.694465Z","iopub.execute_input":"2023-04-10T07:28:58.694916Z","iopub.status.idle":"2023-04-10T07:28:58.702989Z","shell.execute_reply.started":"2023-04-10T07:28:58.694876Z","shell.execute_reply":"2023-04-10T07:28:58.701834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nfolds = 3\n\nfor fold_idx in range(folds):\n    model = get_model(hp)\n    start = time.time()\n\n    lr = 1e-3\n\n    model.compile(\n        tf.keras.optimizers.Adam(learning_rate=lr),\n        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n        metrics=[\n            tf.keras.metrics.SparseCategoricalAccuracy(),\n        ]\n    )\n    \n    if not mock_run:\n        hist = model.fit(\n        x=ds,\n        epochs=60,\n        verbose=2, # 2 = one line per epoch\n        callbacks=[\n            TimeLimitCallback(start, 2, 30), #1 minute max for testing purposes\n            tf.keras.callbacks.LearningRateScheduler(scheduler),\n            tf.keras.callbacks.ModelCheckpoint(pjoin(path.split('/')[0], f\"model_{fold_idx}\"), \n                save_best_only=True, \n                save_weights_only=True,\n                restore_best_weights=True, \n                monitor=\"sparse_categorical_accuracy\", mode=\"max\"),\n            tf.keras.callbacks.EarlyStopping(patience=10, \n             monitor=\"sparse_categorical_accuracy\", mode=\"max\", restore_best_weights=True)\n            ],\n        #validation_data=valid_ds,\n        #validation_freq=1,\n        workers=2,\n        use_multiprocessing=True\n    )","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:28:58.707108Z","iopub.execute_input":"2023-04-10T07:28:58.708329Z","iopub.status.idle":"2023-04-10T07:29:08.399120Z","shell.execute_reply.started":"2023-04-10T07:28:58.708293Z","shell.execute_reply":"2023-04-10T07:29:08.397496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:29:08.400347Z","iopub.status.idle":"2023-04-10T07:29:08.401511Z","shell.execute_reply.started":"2023-04-10T07:29:08.401242Z","shell.execute_reply":"2023-04-10T07:29:08.401269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"440 s cpu","metadata":{}},{"cell_type":"code","source":"def get_inference_model():\n    inputs = tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\")\n    \n    diffs = Preprocess(interpolated_length)(inputs)\n    diffs = {k:tf.expand_dims(v, 0) for k,v in diffs.items()}\n    \n    models = [get_model(hp) for _ in range(folds)]\n    for fold_idx in range(folds):\n        if not mock_run:\n            models[fold_idx].load_weights(pjoin(path.split('/')[0], f\"model_{fold_idx}\"))\n        models[fold_idx]._name += f\"_{fold_idx}\" \n    outputs = [model(diffs) for model in models]\n    vector = tf.reduce_mean(outputs, axis=0) #average the models\n\n    output = tfkl.Activation(activation=\"linear\", name=\"outputs\")(vector)\n    inference_model = tf.keras.Model(inputs=inputs, outputs=output) \n    inference_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n    return inference_model\n\ninference_model = get_inference_model()\n#inference_model.summary()\nparams = inference_model.count_params()\nprint(params/1e6)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:29:08.402938Z","iopub.status.idle":"2023-04-10T07:29:08.403802Z","shell.execute_reply.started":"2023-04-10T07:29:08.403526Z","shell.execute_reply":"2023-04-10T07:29:08.403567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\ntflite_model = converter.convert()\nmodel_path = \"model.tflite\"\n# Save the model.\nwith open(model_path, 'wb') as f:\n    f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:29:08.407807Z","iopub.status.idle":"2023-04-10T07:29:08.408368Z","shell.execute_reply.started":"2023-04-10T07:29:08.408107Z","shell.execute_reply":"2023-04-10T07:29:08.408132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip submission.zip $model_path","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:29:08.410232Z","iopub.status.idle":"2023-04-10T07:29:08.410737Z","shell.execute_reply.started":"2023-04-10T07:29:08.410464Z","shell.execute_reply":"2023-04-10T07:29:08.410489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_DIR = \"/kaggle/input/asl-signs/\"\ntrain_df = pd.read_csv(pjoin(BASE_DIR, \"train.csv\"))\n\npath2label = dict(zip(train_df.path, train_df.sign))\nlabel2int = json.load(open(pjoin(BASE_DIR, \"sign_to_prediction_index_map.json\"), 'rb'))\n\nint2label = {v:k for k,v in label2int.items()}\n\nimport tflite_runtime.interpreter as tflite\ninterpreter = tflite.Interpreter(model_path)\nfound_signatures = list(interpreter.get_signature_list().keys())\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\n#for i in range(len(train_df)):\nfor i in range(50):\n    frames = load_relevant_data_subset(f'/kaggle/input/asl-signs/{train_df.iloc[i].path}')\n    output = prediction_fn(inputs=frames)\n    sign = np.argmax(output[\"outputs\"])\n    print(f\"Predicted label: {int2label[sign]}, Actual Label: {train_df.iloc[i].sign} (shape {frames.shape})\")","metadata":{"execution":{"iopub.status.busy":"2023-04-10T07:29:08.412492Z","iopub.status.idle":"2023-04-10T07:29:08.413140Z","shell.execute_reply.started":"2023-04-10T07:29:08.412888Z","shell.execute_reply":"2023-04-10T07:29:08.412912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}