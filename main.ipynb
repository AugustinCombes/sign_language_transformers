{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "tfkl = tf.keras.layers\n",
    "from tqdm.keras import TqdmCallback\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from preprocess_models.full_length import Preprocess\n",
    "from models.LSTM import get_model\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_relevant_data_subset\n",
    "from preprocess_models.interpolated_ragged import Preprocess\n",
    "from datasets.ragged_sym import get_KFold_dataset\n",
    "from models.gru import get_model\n",
    "\n",
    "folds = 7\n",
    "path = \"preprocessed_datasets/interpolated_ragged_nanfinalmask/\"\n",
    "\n",
    "ds = get_KFold_dataset(Preprocess(10), (None, 122), path)\n",
    "hp = {\n",
    "    \"gru1\": 96,\n",
    "    'nhead': 12,\n",
    "    'ff_dim': 160,\n",
    "    'input_dropout': 0.3,\n",
    "    'gru2': 96,\n",
    "    'output_dropout': 0.2\n",
    "}\n",
    "\n",
    "get_model(hp).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accs = list()  \n",
    "for fold_idx in range(folds):\n",
    "    start = time.time()\n",
    "    print(f'\\n            *** Fold {fold_idx} ***\\n')\n",
    "    train_ds = ds.filter(lambda v, l, g, pid, sid: g != fold_idx).map(detuple).padded_batch(32)\n",
    "    valid_ds = ds.filter(lambda v, l, g, pid, sid: g == fold_idx).map(detuple).padded_batch(32)\n",
    "    model = get_model(hp)\n",
    "\n",
    "    lr = 1e-3\n",
    "\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    hist = model.fit(\n",
    "        x=train_ds,\n",
    "        epochs=100,\n",
    "        verbose=2,\n",
    "        callbacks=[\n",
    "            TqdmCallback(verbose=0),\n",
    "            # tfa.callbacks.TQDMProgressBar(),\n",
    "            # TimeLimitCallback(start, 1, 10), \n",
    "            tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "            tf.keras.callbacks.ModelCheckpoint(pjoin(path.split('/')[0], f\"model_{fold_idx}\"), \n",
    "                save_best_only=True, \n",
    "                save_weights_only=True,\n",
    "                restore_best_weights=True, \n",
    "                monitor=\"val_sparse_categorical_accuracy\", mode=\"max\"),\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_sparse_categorical_accuracy\", mode=\"max\", restore_best_weights=True)\n",
    "            ],\n",
    "        validation_data=valid_ds,\n",
    "        validation_freq=1,\n",
    "        workers=2,\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    best_acc = max(hist.history['val_sparse_categorical_accuracy'])\n",
    "    print(\"Best acc fold\", fold_idx, \":\\n ->\", 100*round(best_acc, 4), \"%\")\n",
    "    val_accs.append(\n",
    "        best_acc\n",
    "    )\n",
    "    # break\n",
    "\n",
    "print(\"Bagged final valid acc score:\")\n",
    "bagged_score = 100*np.round(np.array(val_accs).mean(), 4)\n",
    "print(bagged_score, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "globalement 50 s pour une epoch -> voir effet batch size réduit sur généralisation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 47/100\n",
    "\n",
    "2520/2520 - 52s - loss: 0.5897 - sparse_categorical_accuracy: 0.8558 - val_loss: 1.9944 - val_sparse_categorical_accuracy: 0.5938 - lr: 2.0242e-05 - 52s/epoch - 21ms/step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_handed = p(load_relevant_data_subset('/Users/gus/Desktop/envs/asl/asl-signs/train_landmark_files/16069/1101405492.parquet'))\n",
    "right_handed = p(load_relevant_data_subset('/Users/gus/Desktop/envs/asl/asl-signs/train_landmark_files/28656/55639303.parquet'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "# Define the plot function\n",
    "def plotte(x): \n",
    "    plt.plot(x[:, 0], x[:, 1], 'ro')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "# Create a list to store the plot images\n",
    "images = []\n",
    "\n",
    "for i, im in enumerate(left_handed[:]):\n",
    "    # Create a new plot\n",
    "    plt.figure()\n",
    "    plotte(im)\n",
    "    # Save the plot as an image\n",
    "    plt.savefig(f\"tmp/frame_{i:02d}.png\")\n",
    "    \n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "    # Add the image to the list\n",
    "    images.append(Image.open(f\"tmp/frame_{i:02d}.png\"))\n",
    "\n",
    "# Save the images as a GIF file\n",
    "imageio.mimsave(\"left_handed.gif\", images, fps=5)\n",
    "len(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian search template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_nlp.layers.transformer_encoder import TransformerEncoder\n",
    "from keras_nlp.layers import SinePositionEncoding\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, hp):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hp = hp\n",
    "        self.reshape = tfkl.Reshape((normalized_length, -1))\n",
    "\n",
    "        self.mean_embedder = tf.keras.Sequential([\n",
    "            tfkl.Dense(hp['meandim']),\n",
    "            tfkl.BatchNormalization(),\n",
    "            tfkl.Activation('gelu')\n",
    "        ])\n",
    "\n",
    "        self.std_embedder = tf.keras.Sequential([\n",
    "            tfkl.Dense(hp[\"stddim\"]),\n",
    "            tfkl.BatchNormalization(),\n",
    "            tfkl.Activation('gelu'),\n",
    "            # tfkl.Dropout(0.3),\n",
    "        ])\n",
    "\n",
    "        self.norm = tfkl.BatchNormalization()\n",
    "        self.gelu = tfkl.Activation('gelu')\n",
    "        self.transformer_encoders = [TransformerEncoder(\n",
    "            intermediate_dim=2*hp[\"d_model\"], \n",
    "            num_heads=hp[\"heads\"], \n",
    "            dropout=hp[\"dropout\"]\n",
    "            ) for _ in range(hp[\"modules\"])]\n",
    "\n",
    "        # self.cls_token = tf.Variable(tf.random.normal((1, 1, hp['maindim'])), name='cls')\n",
    "        self.cls_token = self.add_weight(shape=(1, 1, hp['maindim']), initializer='random_normal', name='cls')\n",
    "\n",
    "        self.dropout = tfkl.Dropout(hp[\"dropout2\"])\n",
    "        self.classifier = tfkl.Dense(250, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.reshape(inputs)\n",
    "\n",
    "        b, t, d = x.shape\n",
    "        d = int(d/2)\n",
    "        means = x[:, :, :d]\n",
    "        stds = x[:, :, d:]\n",
    "        # Early fusion of mean & std modalities, (and of hands & lips modalities)\n",
    "        x = tf.concat([self.mean_embedder(means), self.std_embedder(stds)], axis=-1)\n",
    "        \n",
    "        # Append cls token\n",
    "        cls_tokens = tf.tile(self.cls_token, [tf.shape(x)[0], 1, 1])\n",
    "        x = tfkl.Concatenate(axis=1)([x, cls_tokens])\n",
    "\n",
    "        # Add positional encoding\n",
    "        x += SinePositionEncoding()(x)\n",
    "\n",
    "        x = self.norm(x) # ?\n",
    "        # x = self.gelu(x)\n",
    "\n",
    "        # Transformers layers\n",
    "        for encoder in self.transformer_encoders:\n",
    "            x = encoder(x)\n",
    "        \n",
    "        # CLS pooling\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Classification\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_model(hp):\n",
    "    main_dim = hp.Int(name='maindim', min_value=32, max_value=256, step=32)\n",
    "    heads = hp.Int(name='heads', min_value=4, max_value=16, step=4)\n",
    "    dropout_att = hp.Float(name='dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    dropout_emb = hp.Float(name='dropout2', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    lstdim = hp.Int(name='lstmdim', min_value=32, max_value=128, step=32)\n",
    "    n_modules = hp.Int(name='modules', min_value=1, max_value=4, step=1)\n",
    "    return MyModel(hp)\n",
    "\n",
    "model = get_model(hp)\n",
    "model(e[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import BayesianOptimization\n",
    "from keras import backend as backend\n",
    "backend.clear_session()\n",
    "\n",
    "val_accs = list()  \n",
    "for fold_idx in range(folds):\n",
    "    start = time.time()\n",
    "    print(f'\\n            *** Fold {fold_idx} ***\\n')\n",
    "    train_ds = ds.filter(lambda v, l, g: g != fold_idx).map(detuple).batch(32)\n",
    "    valid_ds = ds.filter(lambda v, l, g: g == fold_idx).map(detuple).batch(32)\n",
    "    model = get_model(hp)\n",
    "\n",
    "    lr = 1e-3\n",
    "\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    hist = model.fit(\n",
    "        x=train_ds,\n",
    "        epochs=50,\n",
    "        verbose=2,\n",
    "        callbacks=[\n",
    "            TimeLimitCallback(start, 1, 10), \n",
    "            tf.keras.callbacks.LearningRateScheduler(scheduler),\n",
    "            tf.keras.callbacks.ModelCheckpoint(pjoin(path.split('/')[0], f\"model_{fold_idx}\"), \n",
    "                save_best_only=True, \n",
    "                save_weights_only=True,\n",
    "                restore_best_weights=True, \n",
    "                monitor=\"val_sparse_categorical_accuracy\", mode=\"max\"),\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_sparse_categorical_accuracy\", mode=\"max\", restore_best_weights=True)\n",
    "            ],\n",
    "        validation_data=valid_ds,\n",
    "        validation_freq=1,\n",
    "        workers=2,\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    best_acc = max(hist.history['val_sparse_categorical_accuracy'])\n",
    "    print(\"Best acc fold\", fold_idx, \":\\n ->\", 100*round(best_acc, 4), \"%\")\n",
    "    val_accs.append(\n",
    "        best_acc\n",
    "    )\n",
    "\n",
    "print(\"Bagged final valid acc score:\")\n",
    "bagged_score = 100*np.round(np.array(val_accs).mean(), 4)\n",
    "print(bagged_score, \"%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(\n",
    "    get_model,\n",
    "    objective='val_sparse_categorical_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='',\n",
    "    project_name=path,\n",
    ")\n",
    "\n",
    "tuner.search(train_ds, epochs=50, validation_data=valid_ds, callbacks=cb_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
